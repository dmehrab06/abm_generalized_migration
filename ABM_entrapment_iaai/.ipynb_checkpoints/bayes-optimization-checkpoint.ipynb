{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABM Optimizer is now aware of 10 points.\n",
      "Next point to probe is: {'A': 53.03687408464508, 'D': 4.549275037262223, 'T': 0.8884524088971553, 'b_prob': 0.3464487843737808, 'ews': 0.43206914716162437, 'm_lo': 0.9419415521029405, 'p_hi': 13.532889328595237, 'p_lo': 3.0641860872337374, 'ps': 0.9435306828280509, 't_r': 7.555853529203512}\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n",
    "from bayes_opt.util import load_logs\n",
    "from file_paths_and_consts import *\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from bayes_opt import UtilityFunction\n",
    "import json\n",
    "\n",
    "SEED_VALUE = 590\n",
    "random.seed(SEED_VALUE)\n",
    "\n",
    "#OUTPUT_DIR\n",
    "\n",
    "def get_memory(hh):\n",
    "    if hh<=100000:\n",
    "        return 16000\n",
    "    if hh<=300000:\n",
    "        return 64000\n",
    "    return 256000\n",
    "\n",
    "def get_core(hh):\n",
    "    if hh<=100000:\n",
    "        return 1\n",
    "    if hh<=300000:\n",
    "        return 4\n",
    "    return 8\n",
    "\n",
    "def get_results_for_2(hyper_comb, who='refugee',region_name=[\"all\"],prefix='fresh_calib_batch_simulation',look_until=100,ROLL=7):\n",
    "    \n",
    "    search_in_ids = all_ids\n",
    "    if region_name!=[\"all\"]:\n",
    "        search_in_ids = region_name\n",
    "    \n",
    "    all_dfs = []\n",
    "    found = 0\n",
    "    for cur_id in search_in_ids:\n",
    "        f_name = prefix+'_completed_'+str(cur_id)+'_'+str(hyper_comb).zfill(5)+'.csv'\n",
    "        f2_name = prefix+'_'+str(cur_id)+'_'+str(hyper_comb).zfill(5)+'.csv'\n",
    "        if os.path.isfile(OUTPUT_DIR+f_name):\n",
    "            true_f_name = f_name\n",
    "        elif os.path.isfile(OUTPUT_DIR+f2_name):\n",
    "            true_f_name = f2_name\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        cur_df = pd.read_csv(OUTPUT_DIR+true_f_name)\n",
    "        #print(cur_df.shape[0],end=' ')\n",
    "        cur_df['time'] = pd.to_datetime(cur_df['time'])\n",
    "        \n",
    "        #cur_df = cur_df.sort_values(by=['time',who],ascending=[True,False])\n",
    "        #cur_df = cur_df.drop_duplicates(keep='first')\n",
    "        \n",
    "        all_dfs.append(cur_df)\n",
    "        found = found + 1\n",
    "    \n",
    "    ovr_df = pd.concat(all_dfs)\n",
    "    ovr_df = ovr_df.groupby('time')[who].sum().reset_index()\n",
    "    ovr_df[who] = ovr_df[who].rolling(ROLL).mean()\n",
    "    #print(ovr_df.columns.tolist())\n",
    "    ovr_df = ovr_df.dropna(subset=[who])\n",
    "    print(found,'raions found')\n",
    "    return ovr_df\n",
    "\n",
    "ROLLING = 7\n",
    "refugee_df = pd.read_csv(GROUND_TRUTH_DIR+'ukraine_refugee_data_2.csv')\n",
    "refugee_df['time'] = pd.to_datetime(refugee_df['time'])\n",
    "refugee_df['refugee'] = refugee_df['refugee'].rolling(ROLLING).mean()\n",
    "refugee_df = refugee_df.dropna(subset=['refugee'])\n",
    "\n",
    "cur_household_data = pd.read_csv(HOUSEHOLD_DIR+'ukraine_household_data.csv')\n",
    "id_to_name = cur_household_data[['matching_place_id','matching_place_name']].drop_duplicates()\n",
    "all_ids = []#cur_household_data.matching_place_id.unique().tolist()\n",
    "#id_to_name['matching_place_name'].value_counts()\n",
    "dict_id_to_name = dict(zip(id_to_name.matching_place_id, id_to_name.matching_place_name))\n",
    "\n",
    "neighbor_adm2 = pd.read_csv(UNCLEANED_DATA_DIR+'neighbor_raions.csv')\n",
    "all_ids = all_ids + neighbor_adm2.ADM2_EN_x.unique().tolist()\n",
    "\n",
    "abm_pbounds = {\n",
    "    'D':(1.0,5.0),\n",
    "    'A':(30.0,70.0),\n",
    "    'T':(0.5,3.0),\n",
    "    'S':(0.01,1.0),\n",
    "    #'t_l':(5,21),\n",
    "    't_r':(1,14),\n",
    "    'ps':(0.0,1.0),\n",
    "    'ews':(0.0,1.0),\n",
    "    #'pactive':(0,60),\n",
    "    'p_hi':(20,100),\n",
    "    'p_lo':(0,10),\n",
    "    #'phase_shift':(0,60),\n",
    "    'b_prob':(0.2,0.4),\n",
    "    #'m_lo':(0.7,1.0),\n",
    "    #'m_hi':(1.5,2.0),\n",
    "}\n",
    "\n",
    "network_struct = 13\n",
    "dynamic_network = 1\n",
    "use_neighbor = 5#random.sample([2,5,8,10],1)[0]#random.unifrom(0.0,1.0)\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=None,\n",
    "    pbounds=abm_pbounds,\n",
    "    verbose=2,\n",
    "    random_state=3,\n",
    ")\n",
    "utility = UtilityFunction(kind=\"ucb\", kappa=2.5, xi=0.0)\n",
    "load_logs(optimizer, logs=[\"./logs/bayes_examples_all_log_revised.json\"])\n",
    "print(\"ABM Optimizer is now aware of {} points.\".format(len(optimizer.space)))\n",
    "logger = JSONLogger(path=\"./logs/bayes_examples_current_log.json\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "next_point_to_probe = optimizer.suggest(utility)\n",
    "print(\"Next point to probe is:\", next_point_to_probe)\n",
    "\n",
    "import subprocess\n",
    "subprocess_output = subprocess.run([\"squeue\",\"-u\",\"zm8bh\",\"--name=bayes_peer_int\"],capture_output=True)\n",
    "\n",
    "print(len(str(subprocess_output)))\n",
    "\n",
    "if len(str(subprocess_output))==231:\n",
    "    \n",
    "    if len(optimizer.space)>=0: # after first run change this from >0 to >=0\n",
    "\n",
    "        print('deleting previous runs')\n",
    "        all_files = [f for f in os.listdir(\".\") if f.endswith('.out')]\n",
    "        all_files.sort()\n",
    "        for f in all_files[0:-3]:\n",
    "            os.remove(f)\n",
    "        hyper_comb = len(optimizer.space)+5000\n",
    "        print('job finished for',hyper_comb)\n",
    "        res = get_results_for_2(hyper_comb,prefix='mim_result')\n",
    "        comp_df = res.merge(refugee_df,on='time',how='inner')\n",
    "\n",
    "        comp_df['diff'] = (comp_df['refugee_y']-comp_df['refugee_x'])**2\n",
    "        rmse = ((comp_df[5:10]['diff'].sum()+comp_df[50:55]['diff'].sum())/10)**0.5 #10 data points\n",
    "        #rmse = comp_df['diff'].mean()**0.5 #10 data points\n",
    "        target = -rmse\n",
    "        print(\"Found the target value to be:\", target)\n",
    "\n",
    "        optimizer.register(\n",
    "            params=next_point_to_probe,\n",
    "            target=target,\n",
    "        )\n",
    "\n",
    "        with open('./logs/bayes_examples_current_log.json') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        print(data)\n",
    "        with open('./logs/bayes_examples_all_log_revised.json', \"a\") as f:\n",
    "            f.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "        optimizer = BayesianOptimization(\n",
    "            f=None,\n",
    "            pbounds=abm_pbounds,\n",
    "            verbose=2,\n",
    "            random_state=3,\n",
    "        )\n",
    "        utility = UtilityFunction(kind=\"ucb\", kappa=2.5, xi=0.0)\n",
    "        load_logs(optimizer, logs=[\"./logs/bayes_examples_all_log_revised.json\"])\n",
    "        print(\"ABM Optimizer is now aware of {} points.\".format(len(optimizer.space)))\n",
    "        logger = JSONLogger(path=\"./logs/bayes_examples_current_log.json\")\n",
    "        optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "        next_point_to_probe = optimizer.suggest(utility)\n",
    "        print(\"Next point to probe is:\", next_point_to_probe)\n",
    "\n",
    "        #raion_df = pd.read_csv('/home/zm8bh/radiation_model/migration_shock/scripts/analysis_notebooks/memory_req_raion.csv')\n",
    "\n",
    "    X = next_point_to_probe\n",
    "    hyper_comb = len(optimizer.space)+5000\n",
    "\n",
    "    f = open(\"bayesian_calibration_\"+str(hyper_comb)+\".sh\", \"w\")\n",
    "\n",
    "    D = X['D']\n",
    "    A = X['A']\n",
    "    S = X['S']\n",
    "    T = X['T']\n",
    "    t_r = int(X['t_r'])\n",
    "    t_l = t_r\n",
    "    ps = X['ps']\n",
    "    ews = X['ews']\n",
    "    pactive = 40\n",
    "    peer_thresh_lo = float(X['p_lo'])\n",
    "    peer_thresh_hi = float(X['p_hi'])\n",
    "    border_cross_prob = X['b_prob']\n",
    "    phase_shift = 40\n",
    "    multiply_lo = 1.0\n",
    "    multiply_hi = 1.8\n",
    "    comment = 'Bayes_calibration_fewer_parameters_40_shift_'+str(hyper_comb)\n",
    "\n",
    "    necessary_mem = pd.read_csv('/home/zm8bh/radiation_model/migration_shock/scripts/analysis_notebooks/memory_req_raion.csv')\n",
    "    raion_df = pd.read_csv('hh_cnts.csv')\n",
    "\n",
    "    raion_df = raion_df.sort_values(by='hh',ascending=False)\n",
    "    raion_df['memory'] = raion_df['hh'].apply(lambda x: get_memory(x))\n",
    "    raion_df['core'] = raion_df['hh'].apply(lambda x: get_core(x))\n",
    "    raion_df = raion_df.merge(necessary_mem[['Raion','mem_need']],left_on='raion',right_on='Raion',how='inner')\n",
    "    raion_df['memory'] = raion_df[[\"memory\", \"mem_need\"]].max(axis=1)\n",
    "    raion_df = raion_df.sort_values(by='hh',ascending=False)\n",
    "\n",
    "    raion = raion_df['raion'].tolist()\n",
    "    mem = raion_df['memory'].tolist()\n",
    "    cc = raion_df['core'].tolist()\n",
    "\n",
    "    partition = 1\n",
    "\n",
    "    set1_raion = raion[0:partition]\n",
    "    set1_mem = mem[0:partition]\n",
    "    set1_cc = cc[0:partition]\n",
    "\n",
    "    set2_raion = raion[partition:]\n",
    "    set2_mem = mem[partition:]\n",
    "    set2_cc = cc[partition:]\n",
    "    set2_raion.reverse()\n",
    "    set2_mem.reverse()\n",
    "    set2_cc.reverse()\n",
    "\n",
    "    for i in range(len(set1_raion)):\n",
    "        name = set1_raion[i]\n",
    "        mem_req = set1_mem[i]\n",
    "        core_use = set1_cc[i]\n",
    "        print('sbatch --mem='+str(mem_req)+' --cpus-per-task='+str(core_use)+' ukr_mim_mdm_sample.sbatch',end=' ',file=f)\n",
    "        if name.startswith('Chornobyl'):\n",
    "            print('\"'+name+'\"',hyper_comb,D,A,T,S,t_l,t_r,ps,ews,pactive,peer_thresh_lo,peer_thresh_hi,\n",
    "                  network_struct,dynamic_network,use_neighbor,border_cross_prob,phase_shift,multiply_lo,multiply_hi,core_use,file=f)\n",
    "        else:\n",
    "            print(name,hyper_comb,D,A,T,S,t_l,t_r,ps,ews,pactive,peer_thresh_lo,peer_thresh_hi,\n",
    "                  network_struct,dynamic_network,use_neighbor,border_cross_prob,phase_shift,multiply_lo,multiply_hi,core_use,file=f)\n",
    "\n",
    "\n",
    "    for i in range(len(set2_raion)):\n",
    "        name = set2_raion[i]\n",
    "        mem_req = set2_mem[i]\n",
    "        core_use = set2_cc[i]\n",
    "        print('sbatch --mem='+str(mem_req)+' --cpus-per-task='+str(core_use)+' ukr_mim_mdm_sample.sbatch',end=' ',file=f)\n",
    "        if name.startswith('Chornobyl'):\n",
    "            print('\"'+name+'\"',hyper_comb,D,A,T,S,t_l,t_r,ps,ews,pactive,peer_thresh_lo,peer_thresh_hi,\n",
    "                  network_struct,dynamic_network,use_neighbor,border_cross_prob,phase_shift,multiply_lo,multiply_hi,core_use,file=f)\n",
    "        else:\n",
    "            print(name,hyper_comb,D,A,T,S,t_l,t_r,ps,ews,pactive,peer_thresh_lo,peer_thresh_hi,\n",
    "                  network_struct,dynamic_network,use_neighbor,border_cross_prob,phase_shift,multiply_lo,multiply_hi,core_use,file=f)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    df = pd.DataFrame(columns=['hyper_comb','DISTANCE_DECAY','SIGMOID_SCALAR','SIGMOID_EXPONENT','MEMORY_DECAY',\n",
    "                               'TIME_LEFT','TIME_RIGHT','BIAS_SCALE','EVENT_WEIGHT_SCALE','PEER_AFFECT_ACTIVE_DAY',\n",
    "                               'THRESH_HI','USE_NEIGHBOR','BORDER_CROSS_PROB','THRESH_LO','NETWORK_STRUCTURE','COMMENT',\n",
    "                               'FIRST_PHASE_BORDER_CROSS_SCALE','SECOND_PHASE_BORDER_CROSS_SCALE'])\n",
    "\n",
    "    new_row = {'hyper_comb':hyper_comb,'DISTANCE_DECAY':D,'SIGMOID_SCALAR':A,'SIGMOID_EXPONENT':T,'MEMORY_DECAY':S,'TIME_LEFT':t_l,\n",
    "               'TIME_RIGHT':t_r,'BIAS_SCALE':ps,'EVENT_WEIGHT_SCALE':ews,'PEER_AFFECT_ACTIVE_DAY':pactive,\n",
    "               'THRESH_HI':peer_thresh_hi,'THRESH_LO':peer_thresh_lo,'NETWORK_STRUCTURE':network_struct,\n",
    "               'USE_NEIGHBOR':use_neighbor,'BORDER_CROSS_PROB':border_cross_prob,'FIRST_PHASE_BORDER_CROSS_SCALE':multiply_lo,\n",
    "               'SECOND_PHASE_BORDER_CROSS_SCALE':multiply_hi,'COMMENT':comment}\n",
    "\n",
    "    df = df.append(new_row,ignore_index=True)\n",
    "\n",
    "    df.to_csv('../runtime_log/parameter_comb_bayes.csv', mode='a', index=False, header=False)\n",
    "    \n",
    "    print(\"job started for\",hyper_comb)\n",
    "    submit_output = subprocess.run([\"bash\",\"bayesian_calibration_\"+str(hyper_comb)+\".sh\"],capture_output=True)\n",
    "    print(submit_output)\n",
    "else:\n",
    "    print('exit program')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess_output = subprocess.run([\"squeue\",\"-u\",\"zm8bh\",\"--name=bayes_peer_int\"],capture_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess_output = subprocess.run([\"squeue\",\"-u\",\"zm8bh\",\"--name=bayes_peer_int\"],capture_output=True)\n",
    "\n",
    "print(len(str(subprocess_output)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job finished for 510\n"
     ]
    }
   ],
   "source": [
    "hyper_comb = 500+len(optimizer.space)\n",
    "print('job finished for',hyper_comb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bayesian_calibration_510.sh'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f318979e95d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m510\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bayesian_calibration_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".sh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bayesian_calibration_510.sh'"
     ]
    }
   ],
   "source": [
    "check = 510\n",
    "f = open(\"bayesian_calibration_\"+str(check)+\".sh\", \"r\")\n",
    "\n",
    "s = f.readlines()\n",
    "print(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 raions found\n",
      "Found the target value to be: -59045.73185577153\n"
     ]
    }
   ],
   "source": [
    "res = get_results_for_2(509,prefix='mim_result')\n",
    "comp_df = res.merge(refugee_df,on='time',how='inner')\n",
    "\n",
    "comp_df['diff'] = (comp_df['refugee_y']-comp_df['refugee_x'])**2\n",
    "rmse = ((comp_df[5:10]['diff'].sum()+comp_df[50:55]['diff'].sum())/10)**0.5 #10 data points\n",
    "#rmse = comp_df['diff'].mean()**0.5 #10 data points\n",
    "target = -rmse\n",
    "print(\"Found the target value to be:\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting previous runs\n",
      "CompletedProcess(args=['rm', '*.out'], returncode=1, stdout=b'', stderr=b'rm: cannot remove \\xe2\\x80\\x98*.out\\xe2\\x80\\x99: No such file or directory\\n')\n",
      "job finished for 126\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-34db9cf28672>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mhyper_comb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job finished for'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyper_comb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_results_for_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_comb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mim_result'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcomp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefugee_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-bfae6ddee98a>\u001b[0m in \u001b[0;36mget_results_for_2\u001b[0;34m(hyper_comb, who, region_name, prefix, look_until, ROLL)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0movr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0movr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0movr_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwho\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0movr_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwho\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0movr_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwho\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/biocomplexity/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/biocomplexity/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "if len(str(subprocess_output))==222:\n",
    "    print('deleting previous runs')\n",
    "    #rm_output = subprocess.run([\"rm\",\"*.out\"],capture_output=True)\n",
    "    #print(rm_output)\n",
    "    all_files = [f for f in os.listdir(\".\") if f.endswith('.out')]\n",
    "    for f in all_files[0:-3]:\n",
    "        os.remove(f)\n",
    "    hyper_comb = len(optimizer.space)+500\n",
    "    print('job finished for',hyper_comb)\n",
    "    res = get_results_for_2(hyper_comb,prefix='mim_result')\n",
    "    comp_df = res.merge(refugee_df,on='time',how='inner')\n",
    "    \n",
    "    comp_df['diff'] = (comp_df['refugee_y']-comp_df['refugee_x'])**2\n",
    "    rmse = ((comp_df[5:10]['diff'].sum()+comp_df[50:55]['diff'].sum())/10)**0.5 #10 data points\n",
    "    #rmse = comp_df['diff'].mean()**0.5 #10 data points\n",
    "    target = -rmse\n",
    "    print(\"Found the target value to be:\", target)\n",
    "    \n",
    "    optimizer.register(\n",
    "        params=next_point_to_probe,\n",
    "        target=target,\n",
    "    )\n",
    "    \n",
    "    with open('./logs/bayes_examples_current_log.json') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    print(data)\n",
    "    with open('./logs/bayes_examples_all_log_revised.json', \"a\") as f:\n",
    "        f.write(json.dumps(data) + \"\\n\")\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "        f=None,\n",
    "        pbounds=abm_pbounds,\n",
    "        verbose=2,\n",
    "        random_state=3,\n",
    "    )\n",
    "    utility = UtilityFunction(kind=\"ucb\", kappa=2.5, xi=0.0)\n",
    "    load_logs(optimizer, logs=[\"./logs/bayes_examples_all_log_revised.json\"])\n",
    "    print(\"ABM Optimizer is now aware of {} points.\".format(len(optimizer.space)))\n",
    "    logger = JSONLogger(path=\"./logs/bayes_examples_current_log.json\")\n",
    "    optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)\n",
    "    next_point_to_probe = optimizer.suggest(utility)\n",
    "    print(\"Next point to probe is:\", next_point_to_probe)\n",
    "    \n",
    "    #raion_df = pd.read_csv('/home/zm8bh/radiation_model/migration_shock/scripts/analysis_notebooks/memory_req_raion.csv')\n",
    "\n",
    "    X = next_point_to_probe\n",
    "    hyper_comb = len(optimizer.space)+500\n",
    "\n",
    "    f = open(\"bayesian_calibration_\"+str(hyper_comb)+\".sh\", \"w\")\n",
    "\n",
    "    D = X['D']\n",
    "    A = X['A']\n",
    "    S = 98.67\n",
    "    T = X['T']\n",
    "    t_r = int(X['t_r'])\n",
    "    t_l = t_r\n",
    "    ps = X['ps']\n",
    "    ews = X['ews']\n",
    "    pactive = 40\n",
    "    peer_thresh_lo = int(X['p_lo'])\n",
    "    peer_thresh_hi = int(X['p_hi'])\n",
    "    border_cross_prob = X['b_prob']\n",
    "    phase_shift = 40\n",
    "    multiply_lo = X['m_lo']\n",
    "    multiply_hi = 1.8\n",
    "    comment = 'Bayes_calibration_fewer_parameters_40_shift_'+str(hyper_comb)\n",
    "\n",
    "    necessary_mem = pd.read_csv('/home/zm8bh/radiation_model/migration_shock/scripts/analysis_notebooks/memory_req_raion.csv')\n",
    "    raion_df = pd.read_csv('hh_cnts.csv')\n",
    "\n",
    "    raion_df = raion_df.sort_values(by='hh',ascending=False)\n",
    "    raion_df['memory'] = raion_df['hh'].apply(lambda x: get_memory(x))\n",
    "    raion_df['core'] = raion_df['hh'].apply(lambda x: get_core(x))\n",
    "    raion_df = raion_df.merge(necessary_mem[['Raion','mem_need']],left_on='raion',right_on='Raion',how='inner')\n",
    "    raion_df['memory'] = raion_df[[\"memory\", \"mem_need\"]].max(axis=1)\n",
    "    raion_df = raion_df.sort_values(by='hh',ascending=False)\n",
    "\n",
    "    raion = raion_df['raion'].tolist()\n",
    "    mem = raion_df['memory'].tolist()\n",
    "    cc = raion_df['core'].tolist()\n",
    "\n",
    "    partition = 1\n",
    "\n",
    "    set1_raion = raion[0:partition]\n",
    "    set1_mem = mem[0:partition]\n",
    "    set1_cc = cc[0:partition]\n",
    "\n",
    "    set2_raion = raion[partition:]\n",
    "    set2_mem = mem[partition:]\n",
    "    set2_cc = cc[partition:]\n",
    "    set2_raion.reverse()\n",
    "    set2_mem.reverse()\n",
    "    set2_cc.reverse()\n",
    "\n",
    "    for i in range(len(set1_raion)):\n",
    "        name = set1_raion[i]\n",
    "        mem_req = set1_mem[i]\n",
    "        core_use = set1_cc[i]\n",
    "        print('sbatch --mem='+str(mem_req)+' --cpus-per-task='+str(core_use)+' ukr_mim_mdm_sample.sbatch',end=' ',file=f)\n",
    "        if name.startswith('Chornobyl'):\n",
    "            print('\"'+name+'\"',hyper_comb,D,A,T,S,t_l,t_r,ps,ews,pactive,peer_thresh_lo,peer_thresh_hi,\n",
    "                  network_struct,dynamic_network,use_neighbor,border_cross_prob,phase_shift,multiply_lo,multiply_hi,core_use,file=f)\n",
    "        else:\n",
    "            print(name,hyper_comb,D,A,T,S,t_l,t_r,ps,ews,pactive,peer_thresh_lo,peer_thresh_hi,\n",
    "                  network_struct,dynamic_network,use_neighbor,border_cross_prob,phase_shift,multiply_lo,multiply_hi,core_use,file=f)\n",
    "\n",
    "\n",
    "    for i in range(len(set2_raion)):\n",
    "        name = set2_raion[i]\n",
    "        mem_req = set2_mem[i]\n",
    "        core_use = set2_cc[i]\n",
    "        print('sbatch --mem='+str(mem_req)+' --cpus-per-task='+str(core_use)+' ukr_mim_mdm_sample.sbatch',end=' ',file=f)\n",
    "        if name.startswith('Chornobyl'):\n",
    "            print('\"'+name+'\"',hyper_comb,D,A,T,S,t_l,t_r,ps,ews,pactive,peer_thresh_lo,peer_thresh_hi,\n",
    "                  network_struct,dynamic_network,use_neighbor,border_cross_prob,phase_shift,multiply_lo,multiply_hi,core_use,file=f)\n",
    "        else:\n",
    "            print(name,hyper_comb,D,A,T,S,t_l,t_r,ps,ews,pactive,peer_thresh_lo,peer_thresh_hi,\n",
    "                  network_struct,dynamic_network,use_neighbor,border_cross_prob,phase_shift,multiply_lo,multiply_hi,core_use,file=f)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    df = pd.DataFrame(columns=['hyper_comb','DISTANCE_DECAY','SIGMOID_SCALAR','SIGMOID_EXPONENT','MEMORY_DECAY',\n",
    "                               'TIME_LEFT','TIME_RIGHT','BIAS_SCALE','EVENT_WEIGHT_SCALE','PEER_AFFECT_ACTIVE_DAY',\n",
    "                               'THRESH_HI','USE_NEIGHBOR','BORDER_CROSS_PROB','THRESH_LO','NETWORK_STRUCTURE','COMMENT',\n",
    "                               'FIRST_PHASE_BORDER_CROSS_SCALE','SECOND_PHASE_BORDER_CROSS_SCALE'])\n",
    "\n",
    "    new_row = {'hyper_comb':hyper_comb,'DISTANCE_DECAY':D,'SIGMOID_SCALAR':A,'SIGMOID_EXPONENT':T,'MEMORY_DECAY':S,'TIME_LEFT':t_l,\n",
    "               'TIME_RIGHT':t_r,'BIAS_SCALE':ps,'EVENT_WEIGHT_SCALE':ews,'PEER_AFFECT_ACTIVE_DAY':pactive,\n",
    "               'THRESH_HI':peer_thresh_hi,'THRESH_LO':peer_thresh_lo,'NETWORK_STRUCTURE':network_struct,\n",
    "               'USE_NEIGHBOR':use_neighbor,'BORDER_CROSS_PROB':border_cross_prob,'FIRST_PHASE_BORDER_CROSS_SCALE':multiply_lo,\n",
    "               'SECOND_PHASE_BORDER_CROSS_SCALE':multiply_hi,'COMMENT':comment}\n",
    "\n",
    "    df = df.append(new_row,ignore_index=True)\n",
    "\n",
    "    df.to_csv('../runtime_log/parameter_comb_bayes.csv', mode='a', index=False, header=False)\n",
    "    \n",
    "    print(\"job started for\",hyper_comb)\n",
    "    submit_output = subprocess.run([\"bash\",\"bayesian_calibration_\"+str(hyper_comb)+\".sh\"],capture_output=True)\n",
    "    print(submit_output)\n",
    "else:\n",
    "    print('exit program')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-biocomplexity]",
   "language": "python",
   "name": "conda-env-.conda-biocomplexity-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
